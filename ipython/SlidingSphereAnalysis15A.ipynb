{
 "metadata": {
  "name": "",
  "signature": "sha256:c0696a3d2583cce889c4abf94c975286f4b4a14bf5945c959b0fd256c528e71b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Duplicate of the first Sliding Sphere Analysis - Corrections Made - Evaluating 15A Spheres"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Ensure that we are in the main pdbmap directory and not a subdirectory or elsewhere\n",
      "%cd ../../pdbmap"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Z:\\projects\\pdbmap\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load libraries\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the results file into a pandas data frame\n",
      "import pandas as pd\n",
      "results_fname = 'results/sliding_sphere_15/sliding_sphere_1KG_15A.txt'\n",
      "df = pd.read_table(results_fname,delimiter='\\t')\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# View all column names\n",
      "print df.dtypes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define the population lists to avoid mess below\n",
      "pop_pvals = ['AMR_ASN_pval','AMR_EUR_pval','AMR_AFR_pval','ASN_EUR_pval','ASN_AFR_pval','EUR_AFR_pval']\n",
      "pop_ors   = ['AMR_ASN_obsor','AMR_EUR_obsor','AMR_AFR_obsor','ASN_EUR_obsor','ASN_AFR_obsor','EUR_AFR_obsor']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Quality Control: There are a lot of residues with p-values equal to 1.0\n",
      "# I need to find out what is driving these, but first, we'll remove them\n",
      "# They probably reflect testing under an invalid condition\n",
      "df[pop_pvals] = df[pop_pvals].replace(1,np.nan)\n",
      "# There was a problem found earlier with 3RBN duplication, fixing:\n",
      "df = df[df['structid']!='A']\n",
      "df = df[df['structid']!='B']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First plot the p-values for each population comparison\n",
      "for pop_pval in pop_pvals:\n",
      "    fig = plt.figure(figsize=(10,5))\n",
      "    df[pop_pval].hist(bins=100,range=(0,1))\n",
      "    plt.xlabel(pop_pval)\n",
      "    plt.ylabel('count')\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not a good start, really. These p-values seem to be following three separate distributions in the first, second, and third tertiles (had to look that one up). I also don't see any infalation for p-values near 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How many nominally significant (p<0.05) spheres do we have for each population comparison?\n",
      "totcnt = len(df)\n",
      "print \"Total residues tested: %d\"%totcnt\n",
      "for pop_pval in pop_pvals:\n",
      "    popcnt = len(df[~np.isnan(df[pop_pval])])\n",
      "    sigcnt = len(df[df[pop_pval]<0.05])\n",
      "    print \"%s: %d\\tof %d(%2.2f%%)\"%(pop_pval,sigcnt,popcnt,(float(sigcnt)/popcnt)*100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Confusing p-value distributions aside, we're at least seeing nominally significant p-values in fewer than 5% of tested residues within each population, so there is some evidence that those residues are actually significant and not simply the product of chance. Then again, given the p-value distribution, the likelihood of a null residue having a p-value <0.05 isn't 5%, it's likely less."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's see the p-value distributions from before, but only for our nominally significant residues\n",
      "# First plot the p-values for each population comparison\n",
      "for pop_pval in pop_pvals:\n",
      "    fig = plt.figure(figsize=(10,5))\n",
      "    tdf = df[df[pop_pval]<0.05][pop_pval]\n",
      "    tdf.hist(bins=20,range=(0,0.05))\n",
      "    plt.xlabel(\"Nominally significant %s\"%pop_pval)\n",
      "    plt.ylabel('count')\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Oh, so here's a thought. Since I'm only looking at spheres that contained at least 2 variants in one of the populations, I'm further increasing the dependence of my tests. That might explain a lot of the mini-distributions I'm seeing, since any biases in PDB representation would cause inflation of all p-values derived from that protein/residue/gene/etc.\n",
      "\n",
      "I should be able to check for this effect by doing we talked about during our last meeting. I'll generate a non-redundant set of structures representing all of the proteins mapped in the PDB. I'll choose each structure as the one that contains the most protein sequence for the given protein. If this flattens out my p-value distribution, then I can attribute the effect to PDB bias."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# There are too many significant spheres to look at them at once.\n",
      "# How many unique structures contained significant spheres, for each population comparison?\n",
      "for pop_pval in pop_pvals:\n",
      "    tdf = df[df[pop_pval]<0.05]\n",
      "    uniq = tdf.groupby('unp').size()\n",
      "    print \"%s: %d unique proteins with significant residues\"%(pop_pval,len(uniq))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Okay, I'm going to reproduce the original p-value plots, but I'm only going to plot one p-value per unique protein. To find my significant sphers, I have to plot the smallest p-value for each protein. This will bias my p-value distribution towards lower p-values, but the important question will not be the absolute p-values observed, but whether they flatten into a more uniform distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First plot the p-values for each population comparison\n",
      "for pop_pval in pop_pvals:\n",
      "    fig = plt.figure(figsize=(10,5))\n",
      "    tdf = df.loc[df.groupby('unp')[pop_pval].idxmin()]\n",
      "    tdf[pop_pval].hist(bins=100,range=(0,1))\n",
      "    plt.xlabel(pop_pval)\n",
      "    plt.ylabel('count')\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, I did see the bias towards lower pvalues that I expected, but I didn't observe any flattening of the distribution. This time I'm going to group by both unp and seqid. In a perfect dataset, this would uniquely identify each residue in each protein, and only plot it once, regardless of how many times it appeared in structures. However, because alignment is not included in this query, residues may not necessarily match. Again, I'm going to use the lowest p-value for each residue, but it is more appropriate in this case, since I'm referring to a single amino acid in a protein, and asking what its most differentiated state was, across all structures."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the minimum p-value for each unique protein-residue combination, across all structures\n",
      "for pop_pval in pop_pvals:\n",
      "    fig = plt.figure(figsize=(10,5))\n",
      "    df.groupby(['unp','seqid'])[pop_pval].min().hist(bins=100,range=(0,1))\n",
      "#     tdf[pop_pval].hist(bins=100,range=(0,1))\n",
      "    plt.xlabel(pop_pval)\n",
      "    plt.ylabel('count')\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well...huh. Maybe I'll learn something from the odds ratios (priors as determined by Fisher Exact)\n",
      "It is less clear whether I should be looking at the minimum or maximum for this, since these are\n",
      "unordered sets, so I'll just look at one and then the other."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the maximum OR for each sphere in each population comparison\n",
      "for pop_or in pop_ors:\n",
      "    fig = plt.figure(figsize=(10,5))\n",
      "    df.groupby(['unp','seqid'])[pop_or].max().hist(bins=100,range=(0,10))\n",
      "    plt.xlabel(pop_or)\n",
      "    plt.ylabel('count')\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the minimum OR for each sphere in each population comparison\n",
      "for pop_or in pop_ors:\n",
      "    fig = plt.figure(figsize=(10,5))\n",
      "    df.groupby(['unp','seqid'])[pop_or].min().hist(bins=100,range=(0,10))\n",
      "    plt.xlabel(pop_or)\n",
      "    plt.ylabel('count')\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How many duplicates am I even eliminating with this step?\n",
      "\"%d - %d => %d residues in %d proteins\"%(len(df),len(df.groupby(['unp','seqid'])),len(df)-len(df.groupby(['unp','seqid'])),len(df.groupby('unp')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So I've evaluated 12,543,145 residues in solved structures,\n",
      "and I removed 1,126,458 as duplicates from homo-polymers, duplicate structures, etc,\n",
      "leaving me with 11,416,687 uniquely evaluated residues/spheres in 4,649 unique human proteins.\n",
      "\n",
      "It looks like I have a large bias for odds ratios of 0 and infinity, which corresponds to spheres containing no variation in one of the two populations. If the 0/N is in the same direction as the expected frequencies, then the OR is 0.0, if it is in the opposite, the OR is infinite.\n",
      "\n",
      "So far I'm not liking what I'm seeing, but let's move on to looking at a handful of our best-scoring spheres/proteins. I'll start by sorting the proteins by the number of significant spheres identified in that protein."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sort unique proteins by the number of significant spheres\n",
      "for pop_pval in pop_pvals:\n",
      "    tdf = df[df[pop_pval]<0.05]\n",
      "    uniq = tdf.groupby('unp').size()\n",
      "    uniq.sort(ascending=False)\n",
      "    print \"%s: %d unique proteins with significant residues\"%(pop_pval,len(uniq))\n",
      "    print uniq.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "P15151 is a very interesting result. P15151 is the Poliovirus receptor (gene=PVR).\n",
      "Let's look closer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df[(df['unp'] == 'P15151') & np.any(df[pop_pvals]<0.05) & (df['chain']=='A')].sort('seqid')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Okay, this is still a very interesting result, but its clear that the variant count is being inflated by 1DGI and 1NN8, enormous multi-chain structures of poliovirus bound to the polioreceptor. This may be an incredibly interesting structure to study, but for our general list asking the question \"which proteins have the largest number of significant spheres?\", I need to limit that to non-redundant residues."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sort unique proteins by the number of *unique* significant spheres\n",
      "for pop_pval in pop_pvals:\n",
      "    tdf = df[df[pop_pval]<0.05]\n",
      "    uniq = tdf.groupby('unp').seqid.nunique()\n",
      "    uniq.sort(ascending=False)\n",
      "    print \"\\n%s: %d unique proteins with significant residues\"%(pop_pval,len(uniq))\n",
      "    for i,p in enumerate(uniq.index):\n",
      "        print \"%s\\t%d\\tuniprot.org/uniprot/%s\"%(p,uniq[i],p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now, let's do the same, but sorted by structid instead of protein\n",
      "# Sort unique proteins by the number of *unique* significant spheres\n",
      "for pop_pval in pop_pvals:\n",
      "    tdf = df[df[pop_pval]<0.05]\n",
      "    uniq = tdf.groupby('structid').seqid.nunique()\n",
      "    uniq.sort(ascending=False)\n",
      "    print \"\\n%s: %d unique structures with significant residues\"%(pop_pval,len(uniq))\n",
      "    for i,p in enumerate(uniq.index):\n",
      "        print \"%s\\t%d\"%(p,uniq[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}